{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Sklearn,-Data-preprocessing,-and-KNN\" data-toc-modified-id=\"Sklearn,-Data-preprocessing,-and-KNN-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Sklearn, Data preprocessing, and KNN</a></span></li><li><span><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Separating-Training-and-Test-data\" data-toc-modified-id=\"Separating-Training-and-Test-data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Separating Training and Test data</a></span></li><li><span><a href=\"#Dataset-Transformations-in-Sklearn\" data-toc-modified-id=\"Dataset-Transformations-in-Sklearn-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Dataset Transformations in Sklearn</a></span></li><li><span><a href=\"#Imputer\" data-toc-modified-id=\"Imputer-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Imputer</a></span></li><li><span><a href=\"#StandardScaler\" data-toc-modified-id=\"StandardScaler-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>StandardScaler</a></span><ul class=\"toc-item\"><li><span><a href=\"#StandardScaler-directions:\" data-toc-modified-id=\"StandardScaler-directions:-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>StandardScaler directions:</a></span></li></ul></li><li><span><a href=\"#PCA\" data-toc-modified-id=\"PCA-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>PCA</a></span><ul class=\"toc-item\"><li><span><a href=\"#PCA-directions:\" data-toc-modified-id=\"PCA-directions:-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>PCA directions:</a></span></li></ul></li><li><span><a href=\"#Preparing-our-test-data\" data-toc-modified-id=\"Preparing-our-test-data-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Preparing our test data</a></span></li></ul></li><li><span><a href=\"#KNeighborsRegressor\" data-toc-modified-id=\"KNeighborsRegressor-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>KNeighborsRegressor</a></span></li><li><span><a href=\"#Extension\" data-toc-modified-id=\"Extension-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Extension</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sklearn, Data preprocessing, and KNN\n",
    "Now that we've implimented our own version of KNN, we can be a little bit more lazy and let scikit-learn take care of algorithm implimentation for us. The challenges below will walk you through some using some preprocessing techniques and using Sklearn models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration\n",
       "0  18.0          8         307.0       130.0    3504          12.0\n",
       "1  15.0          8         350.0       165.0    3693          11.5\n",
       "2  18.0          8         318.0       150.0    3436          11.0\n",
       "3  16.0          8         304.0       150.0    3433          12.0\n",
       "4  17.0          8         302.0       140.0    3449          10.5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell to import dependencies!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "cars = pd.read_csv('data/sklearn-auto-mpg.csv')\n",
    "\n",
    "cars.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If you want to store this data as a Python list, you can store a reference to cars.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   18. ,     8. ,   307. ,   130. ,  3504. ,    12. ],\n",
       "       [   15. ,     8. ,   350. ,   165. ,  3693. ,    11.5],\n",
       "       [   18. ,     8. ,   318. ,   150. ,  3436. ,    11. ],\n",
       "       [   16. ,     8. ,   304. ,   150. ,  3433. ,    12. ],\n",
       "       [   17. ,     8. ,   302. ,   140. ,  3449. ,    10.5]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = cars.values\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "## Separating Training and Test data\n",
    "We will use sklearn's built in [data-splitting utility](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
    "\n",
    "1. In the first code cell, we imported the `train_test_split`  utillity from sklearn.\n",
    "2. As we did earlier, we must split the `data` into `X` and `y` ourselves.\n",
    "3. Import `train_test_split` from sklearn.model_selection. After separating the labels from the features, use `train_test_split`, specifing a test_size of 0.2 and a random_state of 42. `train_test_split` uses the random_state parameter to shuffle our data before dividing it.\n",
    "4. `train_test_split` returns  `X_train`, `X_test`, `y_train` and `y_test` values (in that order). Store these results as variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset Transformations in Sklearn\n",
    "Most, if not all, of the built-in sklearn models we will be using today are [*dataset transformations*](http://scikit-learn.org/stable/data_transforms.html). A model is a *dataset transformation* if it has a *fit* method, a *transform* method, and a *fit_transform* method. When we instantiate a new instance of a *dataset transformation*, the first thing we must do is train it on our training data. We do this with the *fit* method. Once we have fit the model, we may use the model to transform our training data, or any new, unseen data. This is what the *transform* method is for. If we want to fit a model to our training data, and simultaneously transform the data based on the learned model, we may use the *fit_transform* method. Since nearly all sklearn models follow this design pattern, once we know the syntax for one model, we pretty much know the syntax for every model. How convenient! \n",
    "\n",
    "## Imputer\n",
    "The first *dataset transformation* we will look at is the imputer. Now that we've separated our data into training data and test data, we have to take care of missing values. If we feed data with missing values directly into sklearn's knn model, it will produce a runtime error. There are a few potential approaches for dealing with missing values. One way would be to entirely remove the observation. This is less than ideal since it throws away otherwise useful information.\n",
    "\n",
    "There are two more commonly used approaches:\n",
    "1. In the first approach, fill in missing values with either\n",
    "    * the mean value of the feature,\n",
    "    * the median value of the feature, or \n",
    "    * the mode value of the feature\n",
    "2. In the second approach, we first use approach number 1, but in addition we add a new, binary feature to our dataset that takes on the value 0 if the feature was NOT missing, and 1 if the feature was missing. That way, our model can learn if the absence of a certain feature is helpful for predicting labels.\n",
    "\n",
    "We will use the [sklearn imputer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html) to take approach 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Check to see if `X_train` or `X_test` contain missing values (hint: use functional tools like filter and map)\n",
    "1. If we have missing values, we should import the imputer from sklearn.preprocessing\n",
    "1. Create a new instance of an imputer. We'd like to replace NaNs and Nones with the mean value of the respective feature. The imputer behaves this way by default, but we can also explicitly set it this way by setting the `missing_values` and `strategy` parameters. Save the imputer to the variable `imp`.\n",
    "1. fit the imputer to `X_train`.\n",
    "1. reassign `X_train` the the result of transforming `X_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## StandardScaler\n",
    "A big flaw in our knn model from yesterday is that the [variance](https://en.wikipedia.org/wiki/Variance) in the weight of a vehicle is far greater than the variance in any of the other features. This means that the weight of a vehicle contributed by far the most to the distance a vehicle was from other vehicles, so our model was far less sensitive to variation in cylinders or horsepower.\n",
    "\n",
    "To combat this issue, we can scale our data. Consider the weight of each vehicle. Suppose the mean weight of a vehicle in our training set is 3000kg, and the standard deviation of the weight variable is 550kg (this means that about 68% of vehicles in our training set fall between 2450kg and 3550kg - i.e. 3000kg +/- 550kg). To scale the weight feature, we subtract 3000kg from the weight of each feature, and then divide by 550kg. This leaves us with data that has a mean of 0 and a standard deviation of 1. If we scale all our features, we will mitigate one of the biggest issues with our naive implimentation of knn.\n",
    "\n",
    "### StandardScaler directions:\n",
    "1. Before using StandardScaler, use matplotlib to create a histogram of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "2. Now, import [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) from sklearn.preprocessing\n",
    "2. Create a new instance of the StandardScaler transformer and save it to a variable __scaler__. If you set *copy = False*, when we tranform the data it will transform the data in place. (*note: this will only work if our data is stored in a numpy array*)\n",
    "2. Use fit_transform to simultaneously fit __scaler__ to the training data and transform it to the scaled version.\n",
    "2. Use matplotlib to create a histogram for each scaled feature. Compare the shape of these histograms to the shapes of the histograms you generated above. Compare the mean and variances, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## PCA\n",
    "We'll perform one last tranformation on our data prior to fitting our knn regression model. PCA, short for Principle Component Analysis, is a dimensionality reduction algorithm. That is, PCA can help us turn our dataset which contains 5 unique features into a dataset with fewer unique features.\n",
    "\n",
    "__Why would we want to do this? Aren't we throwing away useful data?__\n",
    "Well, it sort of depends. Suppose we added another feature to our dataset - weight in lbs. This new feature doesn't give us any extra information, since we already know the weight of each car in kg. To put it differently, dimensionality reduction algorithms identify features that are highly correlated, and help us combine them into a single feature. This means we take up less memory and don't allow highly correlated features to add to the distance between points in our knn model. It also mitigates risk of overfitting our model to the training data.\n",
    "\n",
    "__A high level description of PCA:__ PCA essentially fits an ellipse to our data. An n-dimensional ellipse has n different axes. If we have a dataset with n features, the n axes that PCA finds are called the principle components. PCA orders the principle components by their length. The longer a component is relative to the other axes, the more variation in the dataset that is captured by that component. We may discard sufficiently small components because they don't contribute much information about a data-point. Below is an example of a dataset with two features. Its principle components are overlaid as arrows: ![title](GaussianScatterPCA.svg.png)\n",
    "\n",
    "### PCA directions:\n",
    "1. Import [PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) from sklearn.decomposition\n",
    "1. instantiate a new PCA transformer and save it to the variable __pca__\n",
    "1. fit __pca__ to the training data\n",
    "1. print out the values of pca.components\\_, pca.explained\\_variance\\_, and pca.explained\\_variance\\_ratio\\_\n",
    "1. interpret these values and determine how many principle components to keep\n",
    "1. redo step 2, this time passing the parameter n\\_components = # of components you decided to keep\n",
    "1. fit and transform the model with __X_train__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Preparing our test data\n",
    "We've adequately prepared our training data and are almost ready to fit knn models with out transformed data.. In order for the trained model to make valid predictions for our __X_test__, though, we must first transform __X_test__ with the same models we fit to our training data. Do this in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# KNeighborsRegressor\n",
    "We're finally ready to create our knn models!\n",
    "\n",
    "1. import the KNeighborsRegressor from sklearn.neighbors\n",
    "1. import mean_squared_error from sklearn.metrics\n",
    "2. loop through values k = 0 to k = math.floor(math.sqrt(len(__X_train__)))\n",
    "3. for each value of k:\n",
    "    * instantiate a knn model with n_neighbors = k\n",
    "    * use model.predict to generate an array of predictions for __X_test__\n",
    "    * store the value of k in a list called __ks__, and store the mean_squared_error of the predictions in an array called __errors__\n",
    "3. use matplotlib to plot the value of k vs the mean_square_error produced by the corresponding model. Which value of k works best?\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Extension\n",
    "1. Change-up your preprocessing steps, trying to produce a model with a lower mean squared error. You may:\n",
    "    * Tune the hyperparameters for your existing transformers (hyperparameters are the parameters you pass when you instantiate a new tranformer)\n",
    "    * Switch transformers or use fewer transformers (e.g. remove observations with NaNs instead of using imputer, switch out PCA for [Kernel PCA](http://scikit-learn.org/stable/auto_examples/decomposition/plot_kernel_pca.html))\n",
    "1. Each transformer we use in preprocessing comes with implicit assumptions about how our data is structured/distributed. For each transformer, ask yourself what assumptions we are making about our data in order to conclude that the transformation step will be useful.\n",
    "1. Combine your preprocessing steps and knn predictor into a single sklearn [Pipeline](http://scikit-learn.org/stable/modules/pipeline.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
