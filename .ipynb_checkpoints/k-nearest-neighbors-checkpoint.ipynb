{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>K-Nearest Neighbors: Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span></li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Pandas\" data-toc-modified-id=\"Pandas-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Pandas</a></span></li><li><span><a href=\"#Splitting-Data\" data-toc-modified-id=\"Splitting-Data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Splitting Data</a></span></li><li><span><a href=\"#train_test_split\" data-toc-modified-id=\"train_test_split-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span><code>train_test_split</code></a></span></li></ul></li><li><span><a href=\"#K-Nearest-Neighbors\" data-toc-modified-id=\"K-Nearest-Neighbors-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>K-Nearest Neighbors</a></span><ul class=\"toc-item\"><li><span><a href=\"#Background\" data-toc-modified-id=\"Background-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Background</a></span></li><li><span><a href=\"#Example:-3-Nearest-Neighbors\" data-toc-modified-id=\"Example:-3-Nearest-Neighbors-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Example: 3-Nearest Neighbors</a></span></li><li><span><a href=\"#get_distances\" data-toc-modified-id=\"get_distances-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span><code>get_distances</code></a></span></li><li><span><a href=\"#find_neighbors_labels\" data-toc-modified-id=\"find_neighbors_labels-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span><code>find_neighbors_labels</code></a></span></li><li><span><a href=\"#knn_predict\" data-toc-modified-id=\"knn_predict-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span><code>knn_predict</code></a></span></li></ul></li><li><span><a href=\"#Cost-Functions-and-Hyperparameter-Tuning\" data-toc-modified-id=\"Cost-Functions-and-Hyperparameter-Tuning-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Cost Functions and Hyperparameter Tuning</a></span><ul class=\"toc-item\"><li><span><a href=\"#test_knn\" data-toc-modified-id=\"test_knn-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span><code>test_knn</code></a></span></li></ul></li><li><span><a href=\"#Reflection\" data-toc-modified-id=\"Reflection-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Reflection</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "We'll be working with a data set that contains information about different car models. The data set contains six features of each car: mpg, number of cylinders,  displacement, horsepower, weight, and acceleration. Our goal is to create a model that can use the last five features (i.e. number of cylinders,  displacement, horsepower, weight, and acceleration) to predict the mpg value of a car.\n",
    "\n",
    "We'll use a classic machine learning algorithm, k-nearest neighbors (knn), to build our model. Machine learning libraries like scikit-learn have their own implementations of knn. Instead of relying on a third-party implementation, we'll develop our own version in order to gain deep insight into how it works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before writing our own version of knn, we need to prepare our data. Data preprocessing refers to all of the transformations we apply to our data before training a model. We'll use a few libraries to help us along the way. Run the cell below to import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've come across each of these libraries before, with one exception. Let's talk about pandas now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "Pandas is used primarily to import, manipulate, and analyze dataframes. A dataframe is a 2-dimensional data structure made up of rows and columns. The rows of a dataframe represent the observations/records in a data set, while the columns in a dataframe represent a feature of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the `pd.read_csv` to load the file `data/auto-mpg.csv`. Save the result to the variable `df`.\n",
    "1. Invoke the method `df.head`. This will return the first few rows of the dataframe so we can investigate its structure. The result will display below the cell after running the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from CSV file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the `df.describe` method to create a new dataframe that provides summary statistics for each feature of the `df` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Pandas dataframes give us lots of information about our datasets, like the name of each feature and summary statistics about our datasiet. But as we begin to perform transformations on our data, it would be nice to work with plain numpy arrays. In the cell below, use the `df.values` property to reference a numpy array that holds our dataset. Store our data in the variable `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use all of our data to train our model, we will have no data left over to test how well our model performs. Therefore, we want to separate our model into training data, which we will use to train our model, and test data, which we will use to test how our model is performing.\n",
    "\n",
    "A good default rule is to divide your data into 80% training data and 20% test data, although this may differ based on many factors including the size of your data set.\n",
    "\n",
    "We cannot simply designate the top 80% of rows as our training data, because this could introduce bias. For example, what if the data is presented to us ordered by the date it was collected? To address this, we need to randomly shuffle the rows of our data set before dividing it into training data and test data.\n",
    "\n",
    "In addition to dividing our training data and our test data, we also want to separate the label column (i.e. mpg) from the rest of the columns. Why do we need to do this? After we have trained our model, when we use it to make predictions about the mpg of new cars that aren't in our data set, we won't know what their mpg is. We need to structure our data similarly to how the incoming data will be structured, so we will separate the labels from the rest of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `train_test_split`\n",
    "1. Write a function, `train_test_split`, that accepts three parameters:\n",
    "    * `data`: a 2D numpy array of data, which we will split into 4 parts (more on this later).\n",
    "    * `random_seed`: a seed for our random number generator that will determine how our data is shuffled prior to splitting it.\n",
    "    * `test_size`: the portion of the data set to set aside as test data (between 0 and 1).\n",
    "1. This function should return a tuple, `(X_train, y_train, X_test, y_test)`. These variable names follow ML naming convensions, where X contains feature data (aka independent variables), while y contains label data (aka dependent variables).\n",
    "    * `X_train`: the training data, including every column in `data` except the $0^{th}$ column (since this column contains the labels for our data).\n",
    "    * `y_train`: the corresponding labels for the data points in `X_train`.\n",
    "    * `X_test`: the test data, including every column in `data` except the $0^{th}$ column (this column contains the labels).\n",
    "    * `y_test`: the corresponding labels for the data points in `X_test`.\n",
    "1. If you are lost, there's a suggested series of steps below. Please make a good-faith effort to solve this on your own before consulting the steps!\n",
    "1. The cell below the suggested steps contains a few tests for your function.\n",
    "1. In popular Python ML libraries (Ex: [Keras](https://towardsdatascience.com/deep-learning-for-beginners-practical-guide-with-python-and-keras-d295bfca4487)), `y_train` and `y_test` are usually 1D arrays (Ex: `np.array([4, 8, 12])`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your function train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Scroll for suggested steps for completing `train_test_split`*\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "1. [Make a copy](https://www.afternerd.com/blog/python-copy-list/) of data, so that we don't alter the input when we shuffle it\n",
    "1. Seed the random number generator\n",
    "1. Use np.random.shuffle to shuffle the copy of data\n",
    "1. Use test_size, the length of the data array, and math.floor to calculate which index to split our data at\n",
    "1. Use array slicing to create X_train, y_train, X_test, and y_test\n",
    "1. Return the tuple (X_train, y_train, X_test, and y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = train_test_split(data, random_seed = 42, test_size = 0.2)\n",
    "assert X_train.shape == (325, 5)\n",
    "assert y_train.shape == (325,)\n",
    "assert X_test.shape == (81, 5)\n",
    "assert y_test.shape == (81,)\n",
    "\n",
    "print(X_train[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a vast world of other transformations we can make during the preprocessing phase. We'll explore some of them in the extension challenges in `extension-sklearn.ipynb`.\n",
    "\n",
    "Now that we've split our data into the appropriate parts, we're ready to develop our knn algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbors algorithm is a supervised, instance-based learning algorithm that can be used for classification problems or regression problems. Here we will be using knn for regression, not for classification. Let's pick apart some of these terms:\n",
    "\n",
    "* **Supervised learning algorithm**: An algorithm that learns to make predictions based on labeled training data. Since knn only works for labeled data, it's a supervised algorithm.\n",
    "* **Instance-based learning algorithm**: An algorithm that holds all of the training data in memory in order to make future predictions.\n",
    "* **Classification algorithm**: An algorithm that predicts a discrete label (for instance, an algorithm that predicts whether an picture has a cat in it, a dog in it, or neither).\n",
    "* **Regression algorithm**: An algorithm that predicts a continuous label (for instance, miles per gallon of a car. We can have a car with 25 mpg or 25.01 mpg or 25.001 mpg, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does knn work**? Let's pick a specific value of k as an example, like `k = 5`. The 5-nearest neighbors algorithm holds all of its training data in memory. When we present our 5-nearest neighbors algorithm with an unseen data point ( in our case, a car that isn't in our training data set), the 5-nearest neighbors algorithm will search through the training data to find the 5 most similar cars in the training set. Then, we'll average the miles-per-gallon of the 5 most similar cars in our training data, and we'll predict that as the mpg for our unseen car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: 3-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through how knn works for a specific example. Suppose we want to use the 3-nearest neighbors algorithm to predict the mpg of a car. We have a small training set with 10 observations. For the sake of simplicity, let's assume that our data set has only two features, displacement and horsepower. Here's our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_example = np.array([\n",
    "    [ 320.,  130.],\n",
    "    [ 350.,  165.],\n",
    "    [ 318.,  150.],\n",
    "    [ 330.,  165.],\n",
    "    [ 302.,  140.],\n",
    "    [ 429.,  180.],\n",
    "    [ 454.,  220.],\n",
    "    [ 410.,  215.],\n",
    "    [ 455.,  225.],\n",
    "    [ 390.,  190.]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row represents a different car in our training data. The first column is the displacement, the second column is the horsepower. The corresponding miles per gallon of each car is given by the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_example = np.array([18.,  15.,  15.,  16.,  17.,  15.,  14.,  14.,  14.,  18.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot our training data points as if they were points in a 2-dimensional space:\n",
    "\n",
    "![](./docs/assets/images/ex-scatter-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we want to use our 3-nearest neighbors algorithm to predict the miles per gallon of a car that has displacement 351 cc and 153 horsepower. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_point_example = np.array([351.,  153.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add this point to our plot in red:\n",
    "\n",
    "![](./docs/assets/images/ex-scatter-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3-nearest neighbors algorithm will compare the distance between the unseen data point (in red) and all the remaining training data points. We'll then pick the three closest points in the training data, as pictured below:\n",
    "\n",
    "![](./docs/assets/images/ex-scatter-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These three nearest points correspond to the training data points `[ 350.,  165.]`, `[ 318.,  150.]`, and `[ 330.,  165.]`. We can look up their corresponding labels (i.e. miles per gallon) in the variable `y_train_example`. We see that their corresponding labels in `y_train_example` are the values `15.`, `15.`, and `16.`. The three nearest neighbors algorithm will average these values, and predict that the miles per gallon of the unseen point in red is `15.333`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `get_distances`\n",
    "Let's begin our knn algorithm by writing a function, `get_distances`, that computes the distance between an unseen data point and each data point in the training set.\n",
    "\n",
    "We want to write a function that will work for datasets that have two features, three features, four features, or arbitrarily many features. While it's hard (maybe impossible?) to visualize the distance between data points in a high-dimensional data set, the way we calculate the distance is very similar. Let's turn back to our two-dimensional example from above.\n",
    "\n",
    "Consider the plot below. How would we calculate the distance between the red point, `unseen_point_example = [351., 153.]`, and the green point, `a_training_data_point = [390., 190.]`?\n",
    "\n",
    "![](./docs/assets/images/distance-scatter-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we have to do is calculate the length of the purple arrow. So our first step is to find the purple arrow. Notice that the purple arrow starts at the x-value `351` and ends at the x-value `390`. Therefore the purple arrow moves `390 - 351 = 39` units to the right. Also notice that the arrow starts at the y-value `153` and ends at the y-value `190`. Therefore the purple arrow moves `190 - 153 = 37` units up. We can therefore describe the arrow as the point (or more accurately, *vector*) `purple_arrow = [390 - 351, 190 - 153] == [39, 37]`.\n",
    "\n",
    "The way we found the value of the purple arrow was by *component-wise subtraction*! We subtract each component of `unseen_point_example` from each corresponding component of `a_training_point_example`. Using numpy, we can simply write: `purple_arrow = a_training_point_example - unseen_point_example`.\n",
    "\n",
    "Now that we know the value of this purple arrow/vector, we can compute its length. The length of a vector $[x_1, x_2, \\ldots, x_n]$ is given by the formula: \n",
    "\n",
    "$$\\sqrt{\\sum_{i=1}^n x_i^2}$$\n",
    "\n",
    "Here, the term $\\sum_{i=1}^n x_i^2$ means take the sum of each value $x_i^2$ for all values of $i$ between $1$ and $n$. In our 2-dimensional case, we can compute the length of the purple arrow as follows: $\\sqrt{39^2 + 37^2} \\approx 53.76$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a function, `get_distances`, that accepts two parameters:\n",
    "    * `X_train`: a 2-dimensional numpy array that holds a set of training data points\n",
    "    * `unseen_point`: a 1-dimensional numpy array that represents a single data point that isn't in the training set\n",
    "1. `get_distances` should return:\n",
    "    * `distances`: a 1-dimensional numpy array that represents a list of distances. For example, the $5^{th}$ element in `distances` should be the distance between `unseen_point` and the $5^{th}$ data point in `X_train`.\n",
    "1. If you are lost, there's a suggested series of steps below. Please make a good-faith effort to solve this on your own before consulting the steps!\n",
    "1. The cell below the suggested steps contains a few tests for your function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your get_distances function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Scroll for suggested steps for completing `get_distances`*\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "1. Use numpy broadcasting and subtraction to compute an array of arrows like purple arrow in our example. Save the result to a variable, `arrows`.\n",
    "1. Use numpy broadcasting to square every value in `arrows`. Save the result to a variable, `arrows_components_squared`.\n",
    "1. Use `np.sum` to [sum the numbers](https://numpy.org/doc/stable/reference/generated/numpy.sum.html) in each row of `arrows_components_squared`. You may need to pass a value for the keyword argument `axis`. Save the result to a variable, `sum_of_distances_squared`.\n",
    "1. Use `np.sqrt` to take the square root of each value in `sum_of_distances_squared`. return this result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_distances = get_distances(X_train_example, unseen_point_example)\n",
    "desired_distances = np.array([\n",
    "    38.60051813,\n",
    "    12.04159458,\n",
    "    33.13608305,\n",
    "    24.18677324,\n",
    "    50.69516742,\n",
    "    82.54089895,\n",
    "    122.87391912,\n",
    "    85.58621384,\n",
    "    126.49110641,\n",
    "    53.75872022\n",
    "])\n",
    "\n",
    "np.testing.assert_array_almost_equal(example_distances, desired_distances, decimal=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `find_neighbors_labels`\n",
    "1. Define a function `find_neighbors_labels` that accepts three parameters:\n",
    "    * `distances`: a 1-dimensional array of distances (i.e. the output of `get_distances`)\n",
    "    * `y_train`: an array of labels (i.e. an array of mpg's. For example, the $5^{th}$ element of `y_train` is the mpg of the $5^{th}$ data point in `X_train`)\n",
    "    * `k`: The number of nearest neighbors to search for\n",
    "1. `find_neighbors_labels` should return a 1-dimensional numpy array that contains the `k` elements from `y_train` corresponding to the `k` smallest distances in `distances`. *Hint: Look into `np.argsort`*\n",
    "1. As usual, if you are lost, there are suggested steps below.\n",
    "1. The code cell below the suggested steps provides test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your find_neighbors_labels function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Scroll for suggested steps for completing `find_neighbors_labels`*\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "1. Use `np.argsort` to create an array of the indeces of `distances` that correspond with a sorted version of `distances`. (e.g. `np.argsort([50,2,11, 5, 80]) == [1, 3, 2, 0, 4]`). Save the resulting indices of min to max distances to a variable, `indices`.\n",
    "1. Use array slicing to create a copy of `y_train` that is sorted by the distance of the corresponding training point from the unseen point. Save the result to a variable, `y_sorted_by_distance`.\n",
    "    * As an example of array slicing, consider `indices = np.array([1, 3, 2, 0, 4])` and `y_train = np.array([23, 48, 81, 17, 5])`. Then the array `y_train[indices] == [48, 17, 81, 23, 5]`.\n",
    "1. Return the first `k` elements of `y_sorted_by_distance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_neighbors_labels = find_neighbors_labels(example_distances, y_train_example, 3)\n",
    "np.testing.assert_array_equal(example_neighbors_labels, [15, 16, 15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `knn_predict`\n",
    "1. define a function `knn_predict` that will receive an array `X_train`, an array `y_train`, an array `X_test`, and a value `k`\n",
    "2. `knn_predict` should use the functions `get_distances` and `find_neighbors_labels` to predict labels for each observation in __X_test__ using __k__ neighbors. It should return an array of predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your knn_predict function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Functions and Hyperparameter Tuning\n",
    "\n",
    "Now that we have `knn_predict` working, we can produce a number of different models to make predictions about the mpg of a car based on its other features (i.e. we can make a 1-nearest-neighbor model, a 2-nearest-neighbors model, a 3-nearest-neighbors model, etc.)\n",
    "\n",
    "We'd like to teach a machine to use an optimal number of neighbors. To do this, we need a way to for our machine to assess how bad a model is. We'd like to ask the question, \"which of our possible models is the least bad?\".\n",
    "\n",
    "A function that assesses how good or bad a model is at making predictions is called a *cost function* or a *loss function*.  There are many cost functions we could pick from, depending on the type of model and type of data we have. For our purposes, we will stick to the *Mean Squared Error*, or *mse*. \n",
    "\n",
    "To calculate a model's mse, we use a model to predict the labels for each of the observations in our test data set. We then calculate the square of the difference between our prediction and the actual value of the label. This is the squared error for a single observation. The __mse__ is the mean of the squared errors for each observation in our test data set.\n",
    "\\begin{equation*}\n",
    "MSE=\\frac{1}{n}\\sum_{i=1}^n (Y_{pred}-Y_{actual})^2,\\ where\\\\ Y_{pred}=predicted\\ values,\\\\  Y_{actual}=actual\\ observed\\ values,\\\\ n = number\\ of\\ data\\ points\n",
    "\\end{equation*}\n",
    "\n",
    "1. Define a function `mse` that receives an array `y_predicted` and an array `y_test`\n",
    "2. `mse` should return sum of the squared differences, divided by the total number of items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your mse function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test_knn\n",
    "1. define a function `test_knn` that receives `X_train`, `y_train`, `X_test`, and `y_test`  arrays and a `max_neighbors` value\n",
    "2. `test_knn` should compute the mse of the knn model for all values of K, and then return the value for k that produced the lowest mse value.\n",
    "3. Test your model on the training set with the same k and compare that to the result to the test set, which is higher? why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your test_knn function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best number of neighbors:\", test_knn(X_train, y_train, X_test, y_test, max_neighbors=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "1. What is the time complexity and space complexity of knn_predict (where we take input size to be the number of observations in the training data, and all else remains constant)?\n",
    "2. Suppose, in some hypothetical world, the number of cylinders in an engine was highly correlated to the mpg of a car, while the weight of a car (in kg) had very low correlation to mpg. Why will our measure of distance obscure this correlation?\n",
    "3. What are some approaches to fixing the problem described in 2? Try looking into data scaling to improve our model.\n",
    "4. What will happen to our predictions from `knn_predict` as k gets close to the size of our training set?\n",
    "5. Is overfitting more likely to be a problem for large or small values of k?\n",
    "6. How could we use KNN for classification problems instead of regression problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the right K value\n",
    "1. K must not be multiples of the number of classes\n",
    "1. K should not be too small or too large\n",
    "1. K should be an odd number\n",
    "1. Given N total number of samples, the optimal K value is usually:\n",
    "\\begin{equation*}\n",
    "K \\approx \\frac{\\sqrt{N}}{2}\n",
    "\\end{equation*}\n",
    "1. In our example, `N=len(df.values)=406`, `K=math.sqrt(406)/2=10.07`. K cannot be a multiple of 6 and is most likely an odd number close to 10. Hence, K is most likely one of these numbers: `7, 9, 11, 13`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "K-Nearest Neighbors: Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
